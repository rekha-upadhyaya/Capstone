{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>\n",
    "\n",
    "# CDF Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Table of Contents**\n",
    "---\n",
    "**1.** [**Introduction**](#Section1)<br>\n",
    "\n",
    "**2.** [**Problem Statement**](#Section2)<br>\n",
    "\n",
    "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
    "  - **3.1** [**Installing Libraries**](#Section31)\n",
    "  - **3.2** [**Upgrading Libraries**](#Section32)\n",
    "  - **3.3** [**Importing Libraries**](#Section33)\n",
    "\n",
    "**4.** [**Data Acquisition & Information**](#Section4)<br>\n",
    "  - **4.1** [**Data Acquisition**](#Section41)\n",
    "   - **4.1.1** [**Importing Events Dataset from CSV file**](#Section411)\n",
    "   - **4.1.2** [**Importing Gender_age & Brand_model Dataset from MySQL**](#Section412)\n",
    "  - **4.2** [**Data Information**](#Section42)\n",
    "   - **4.2.1** [**Data Information for Events Dataset**](#Section421)\n",
    "   - **4.2.2** [**Data Information for Gender_age Dataset**](#Section422)\n",
    "   - **4.2.3** [**Data Information for Brand_model Dataset**](#Section423)\n",
    "\n",
    "**5.** [**Data Pre-processing**](#Section5)<br>\n",
    "  - **5.1** [**Filtering Events Dataset by States**](#Section51)\n",
    "  - **5.2** [**Merging all three datasets**](#Section52)\n",
    "  - **5.3** [**Pre-Profiling Report**](#Section51)\n",
    "  - **5.4** [**Handling of Missing Data**](#Section52)<br>\n",
    "  - **5.3** [**Feature Engineering.**](#Section53)<br>\n",
    "  - **5.4** [**Post Processing Report**](#Section54)<br>\n",
    "\n",
    "**6.** [**Exploratory Data Analysis**](#Section6)<br>\n",
    "\n",
    "**7.** [**Post Data Processing & Feature Selection**](#Section7)<br>\n",
    "  - **7.1** [**Feature Selection**](#Section71)<br>\n",
    "  - **7.2** [**Encoding the Categorical Data**](#Section72)<br>\n",
    "  - **7.3** [**Data Preparation**](#Section73)<br>\n",
    "\n",
    "**8.** [**Model Development & Evaluation**](#Section8)<br>\n",
    "  - **8.1** [**ModelName - Baseline Model**](#Section81)<br>\n",
    "  - **8.2** [**Using Trained Model for Prediction**](#Section82)<br>\n",
    "  - **8.3** [**Model Evaluation**](#Section83)<br>\n",
    "\n",
    "**9.** [**Summarization**](#Section9)<br>\n",
    "  - **9.1** [**Conclusion**](#Section91)<br>\n",
    "  - **9.2** [**Actionable Insights**](#Section92)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name = Section1></a>\n",
    "# **1. Introduction**\n",
    "---\n",
    "\n",
    "\n",
    "<center><img src=\"\" /></center>\n",
    "\n",
    "InsaidTelecom, one of the leading telecom players, understands that customizing offering is very important for its business to stay competitive.\n",
    "Currently, InsaidTelecom is seeking to leverage behavioral data from more than 60% of the 50 million mobile devices active daily in India to help its clients better understand and interact with their audiences.\n",
    "\n",
    "In this consulting assignment, Insaidians are expected to build a dashboard to understand user's demographic characteristics based on their mobile usage, geolocation, and mobile device properties.\n",
    "\n",
    "Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name = Section2></a>\n",
    "# **2. Problem Statement**\n",
    "---\n",
    "\n",
    "\n",
    "<center><img src=\"\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name = Section3></a>\n",
    "# **3. Installing & Importing Libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section31></a>\n",
    "### **3.1 Installing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 1.1.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "sphinx 4.0.1 requires MarkupSafe<2.0, but you have markupsafe 2.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --user datascience                   \n",
    "!pip install -q --user pandas-profiling              \n",
    "!pip install -q --user yellowbrick                   \n",
    "!pip install mysql-connector-python\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install deep-translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section32></a>\n",
    "### **3.2 Upgrading Libraries**\n",
    "\n",
    "- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. \n",
    "\n",
    "- Make sure not to execute the cell above (3.1) and below (3.2) again after restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pandas-profiling\n",
    "!pip install -q --upgrade yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section33></a>\n",
    "### **3.3 Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd                                                 # Importing for panel data analysis\n",
    "from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis) \n",
    "pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high      \n",
    "pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity      \n",
    "pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n",
    "pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)         # To suppress scientific notation over exponential values\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
    "from scipy.stats import randint as sp_randint                       # for initializing random integer values\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib\n",
    "from matplotlib.pylab import rcParams                               # Backend used for rendering and GUI integration                                               \n",
    "import seaborn as sns                                               # Importin seaborm library for interactive visualization\n",
    "%matplotlib inline\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score                          # For calculating the accuracy for the model\n",
    "from sklearn.metrics import precision_score                         # For calculating the Precision of the model\n",
    "from sklearn.metrics import recall_score                            # For calculating the recall of the model\n",
    "from sklearn.metrics import precision_recall_curve                  # For precision and recall metric estimation\n",
    "from sklearn.metrics import confusion_matrix                        # For verifying model performance using confusion matrix\n",
    "from sklearn.metrics import f1_score                                # For Checking the F1-Score of our model  \n",
    "from sklearn.metrics import roc_curve                               # For Roc-Auc metric estimation\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split                # To split the data in training and testing part     \n",
    "from sklearn.feature_selection import SelectFromModel               # To perform Feature Selection over model\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings                                                     # Importing warning to disable runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import mysql.connector as connection\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name = Section4></a>\n",
    "# **4. Data Acquisition & Information**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section41></a>\n",
    "### **4.1 Data Acquisition**\n",
    "\n",
    "- In this section we will read the datasets from the various sources available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section411></a>\n",
    "#### **4.1.1 Importing 1st dataset - 'events_data' from a csv file**\n",
    "\n",
    "- When a user uses mobile on INSAID Telecom network, the event gets logged in this data. Each event has an event id, location (lat/long), and the event corresponds to frequency of mobile usage. Timestamp tells when the user is using the mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the events data csv file\n",
    "df_events_data = pd.read_csv('C:/Users/supadhyaya8/OneDrive - DXC Production/Documents/cdf/events_data.csv')\n",
    "df_events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the dataset\n",
    "df_events_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section411></a>\n",
    "#### **4.1.2 Importing 2nd & 3rd dataset - 'gender_age_train' & 'phone_brand_device_model' from MySQL database**\n",
    "\n",
    "- gender_age_train           :- Device_ids and their respective user gender, age and age_group\n",
    "- phone_brand_device_model   :- device ids, brand, and device's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the data from the MySQL database for gender_age_train & phone_brand_device_model onto Python by connecting to the below provided MySQL instance.\n",
    "try:\n",
    "    mydb = connection.connect(host=\"cpanel.insaid.co\", database = 'Capstone1',user=\"student\", passwd=\"student\",use_pure=True)\n",
    "    query1 = \"Select * from gender_age_train;\"\n",
    "    query2 = \"Select * from phone_brand_device_model;\"\n",
    "    df_gender_age = pd.read_sql(query1,mydb)\n",
    "    df_brand_model = pd.read_sql(query2,mydb)\n",
    "    mydb.close() #close the connection\n",
    "except Exception as e:\n",
    "    mydb.close()\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getting the head for the gender_age dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getting the head for the brand_model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gender_age.to_csv('gender_age_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.to_csv('phone_brand_device_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section42></a>\n",
    "### **4.2 Data Information**\n",
    "\n",
    "- In this section we will see the **information about the types of features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section421></a>\n",
    "#### **4.2.1 Data Information for Events Dataset**\n",
    "\n",
    "- In this section we will see the **information about the types of features for events dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if null values are present in events dataset\n",
    "df_events_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Events Dataset:**\n",
    "1. There are __3252950 records and 7 features__ in the events dataset.\n",
    "2. There are __453 missing values__ for deviec_id.\n",
    "3. Datatype of __device_id is float__\n",
    "4. __Timestamp__ is object to be converted __to datetime.__\n",
    "5. longitude and latitude have __423 missing values.__\n",
    "6. State has __377 missing values.__\n",
    "7. There are __4 numerical features, 2 categorical features and a timestamp.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section422></a>\n",
    "#### **4.2.2 Data Information for Gender_age Dataset**\n",
    "\n",
    "- In this section we will see the **information about the types of features for gender_age dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age['device_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Gender_age Dataset:**\n",
    "1. There are __74645 records and 4 features__.\n",
    "2. There are __no missing values__.\n",
    "3. __Correct Datatype__ of all the features.\n",
    "4. There are __2 numerical features, 2 categorical features.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section423></a>\n",
    "#### **4.2.3 Data Information for Brand_Model Dataset**\n",
    "\n",
    "- In this section we will see the **information about the types of features for brand_model dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model['device_id'].nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Brand_model Dataset:**\n",
    "1. There are __87726 records and 3 features__.\n",
    "2. There are __no missing values__.\n",
    "3. __Correct Datatype__ of all the features.\n",
    "4. There is __1 numerical feature, 2 categorical features.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating phone brands name in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_translator = GoogleTranslator()\n",
    "\n",
    "\n",
    "#df_brand_model['phone_brand_english'] = df_brand_model['phone_brand'].apply(translator.translate).apply(getattr, args=('text',))\n",
    "\n",
    "df_brand_model['phone_brand_english']= df_brand_model['phone_brand']\n",
    "df_brand_model['device_model_english']= df_brand_model['device_model']\n",
    "\n",
    "# Translating phone_brand\n",
    "brand_translations = {}\n",
    "brand_unique_elements = df_brand_model['phone_brand'].unique()\n",
    "for brand_element in brand_unique_elements:\n",
    "# add translation to the dictionary\n",
    "    brand_translations[brand_element] = g_translator.translate(brand_element, src='zh-cn')    \n",
    "df_brand_model['phone_brand_english'].replace(brand_translations, inplace = True)\n",
    "df_brand_model['phone_brand_english'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brand_model.loc[df_brand_model['device_model'].idxmax()]\n",
    "df_brand_model['device_model'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Translating device_model\n",
    "device_model_element_translations = {}\n",
    "device_model_unique_elements = df_brand_model['device_model'].unique()\n",
    "for device_model_element in device_model_unique_elements:\n",
    "# add translation to the dictionary\n",
    "    device_model_element_translations[device_model_element] = g_translator.translate(device_model_element)    \n",
    "df_brand_model['device_model_english'].replace(device_model_element_translations, inplace = True)\n",
    "df_brand_model['device_model_english'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section5></a>\n",
    "\n",
    "---\n",
    "# **5. Data Pre-Processing**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section51></a>\n",
    "### **5.1 Filtering the events dataset by states (WestBengal, Karnataka, Bihar, Punjab,Gujarat, Kerala)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For consulting, team is to focus on 6 states: WestBengal, Karnataka, Bihar, Punjab,Gujarat and Kerala.\n",
    "- We observed that Events dataset has 377 missing values in 'state' column.\n",
    "- So, first we have to handle these missing values before we filter the dataset by states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data[df_events_data['state'].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Handling the missing value in 'state' column for WestBengal, Karnataka, Bihar, Punjab,Gujarat, Kerala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the unique states\n",
    "df_events_data['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding out the cities for the missing state so that we can fill the missing states from the corresponding city.\n",
    "(df_events_data['city'][df_events_data['state'].isnull()]).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We will fill the missing states for these 3 cities:\n",
    " - Channapatna -> Karnataka\n",
    " - Gangarampur -> WestBengal\n",
    " - Arambagh -> WestBengal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filling the missing values for the states**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the nan values in state with Karnataka where city is Channapatna\n",
    "df_events_data.loc[(df_events_data['city'] == 'Channapatna') & (df_events_data['state'].isnull()), 'state'] = 'Karnataka'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the nan values in state with WestBengal where city is Gangarampur\n",
    "df_events_data.loc[(df_events_data['city'] == 'Gangarampur') & (df_events_data['state'].isnull()), 'state'] = 'WestBengal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the nan values in state with WestBengal where city is Arambagh\n",
    "df_events_data.loc[(df_events_data['city'] == 'Arambagh') & (df_events_data['state'].isnull()), 'state'] = 'WestBengal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-checking if events dataset contains any missing states for WestBengal, Karnataka, Bihar, Punjab,Gujarat and Kerala \n",
    "#by checking the list of cities for the missing states.\n",
    "(df_events_data['city'][df_events_data['state'].isnull()]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data['state'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only 56 missing states belonged to WestBengal, Karnataka, Bihar, Punjab,Gujarat, Kerala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filtering the events database by states (WestBengal, Karnataka, Bihar, Punjab,Gujarat, Kerala)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered = df_events_data[df_events_data['state'].isin(['WestBengal', 'Karnataka', 'Bihar', 'Punjab','Gujarat', 'Kerala'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values in the filtered dataset\n",
    "df_events_data_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for the filtered event dataset:**\n",
    "- There are __422971 records and 7 features__ in the filtered events dataset.\n",
    "- __event_id, timestamp, city and state__ columns have __no missing values.__\n",
    "- __device_id__ has __48 missing values__\n",
    "- __longitude and latitude__ have __42missing values.__\n",
    "- __Data types__ for all the columns are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section52></a>\n",
    "### **5.2 Merging the filtered events dataset with gender_age and brand_model datasets**\n",
    "\n",
    "- Here, we will map all the records of **_events_** dataset with **_gender_age_** and **_brand_model_** datasets to get the demographic details and the brand model details of the users.\n",
    "- We will create a new merged dataframe which will be used for further analysis.\n",
    "- Merging of the datasets will happen on the common column _device id_.\n",
    "- So, first we need to fill in the missing values for the _device id_ in the filtered events dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Handling missing device_ids in the filtered dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered[df_events_data_filtered['device_id'].isnull()].groupby(['longitude', 'latitude','city','state']).count() #sort_values(['state','city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that there are 3 device_ids whose values are missing:\n",
    "- They can be mapped as:\n",
    "  - 1st device id where longitude = 73.16934, latitude = 21.19428, city= Bardoli , state=Gujarat\n",
    "  - 2nd device id where longitude = 70.68639, latitude = 21.79069, city= Jetpur, state=Gujarat\n",
    "  - 3rd device id where longitude = 75.99255, latitude = 31.56175, city= Hoshiarpur, state=Punjab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1st device id\n",
    "df_events_data_filtered[(df_events_data_filtered['longitude'] == 73.16934) & (df_events_data_filtered['latitude'] == 21.19428)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1st device id where longitude = 73.16934, latitude = 21.19428, city= Bardoli , state=Gujarat\n",
    "df_events_data_filtered[(df_events_data_filtered['city'] == 'Bardoli') & (df_events_data_filtered['state'] == 'Gujarat')].groupby(['device_id','longitude', 'latitude','city','state']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1st device id is -8215770519233685504 where longitude = 73.16934, latitude = 21.19428, city= Bardoli , state=Gujarat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 2nd device id where longitude = 70.68639, latitude = 21.79069, city= Jetpur, state=Gujarat\n",
    "df_events_data_filtered[(df_events_data_filtered['city'] == 'Jetpur') & (df_events_data_filtered['state'] == 'Gujarat')].groupby(['device_id','longitude', 'latitude','city','state']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 2nd device id is -1688015122502424064 where longitude = 70.68639, latitude = 21.79069, city= Jetpur, state=Gujarat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered[(df_events_data_filtered['city'] == 'Hoshiarpur') & (df_events_data_filtered['state'] == 'Punjab')].groupby(['device_id','longitude', 'latitude','city','state']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3rd device id is 1750778632182066944 where longitude = 75.99255, latitude = 31.56175, city= Hoshiarpur, state=Punjab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filling the missing values for device ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the missing device id for 1st missing device\n",
    "df_events_data_filtered.loc[((df_events_data_filtered['city'] == 'Bardoli') & (df_events_data_filtered['state'] == 'Gujarat')) & (df_events_data['device_id'].isnull()), 'device_id'] = -8215770519233685504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the missing device id for 2nd missing device\n",
    "df_events_data_filtered.loc[((df_events_data_filtered['city'] == 'Jetpur') & (df_events_data_filtered['state'] == 'Gujarat')) & (df_events_data['device_id'].isnull()), 'device_id'] = -1688015122502424064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the missing device id for 3rd missing device\n",
    "df_events_data_filtered.loc[((df_events_data_filtered['city'] == 'Hoshiarpur') & (df_events_data_filtered['state'] == 'Punjab')) & (df_events_data['device_id'].isnull()), 'device_id'] = 1750778632182066944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_data_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1st Merging the filtered event dataset with gender_age dataset on** _device_id_\n",
    "   - First, we need to convert the data type of __device_id__ in gender_age dataset to __float__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age['device_id'] = df_gender_age['device_id'].astype(df_events_data_filtered['device_id'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Merging the two datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_gender = pd.merge(df_events_data_filtered, df_gender_age, on='device_id', how='left')\n",
    "df_events_gender.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_gender.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_gender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_gender['device_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2nd Merging the event_gender dataset with brand_model dataset on** _device_id_.\n",
    "   - First, we need to convert the data type of __device_id__ in brand_model dataset to __float__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model['device_id'] = df_brand_model['device_id'].astype(df_events_gender['device_id'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Merging the two datasets to get final merged dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged = pd.merge(df_events_gender, df_brand_model, on='device_id', how='left')\n",
    "df_final_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations of the final merged dataset:**\n",
    "- There are __422971 records and 12 features.__\n",
    "- __longitude and latitude__ columns have 42 missing values each. There are __84 missing cells.__\n",
    "- __Timestamp is object__ type needs to be converted __to datetime.__\n",
    "- Data types for all other columns are correct.\n",
    "- There are __6 categorical columns, 5 numerical columns and a timestamp.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- __event id is unique as expected.__\n",
    "- __device id has 19032 distinct values__ and no missing or zero values present.\n",
    "- __Age: Minimum age is 11 and maximum age is 88.__ \n",
    "- __Age distribution is right skewed__ since the mean is to the right of median.\n",
    "- __50 % of the users are between the age 25-36 years. 75% of the users are less than 36 years of age.__ \n",
    "- From 75% (36 yrs) to max age (88) the distribution is widely spread out. There seems to be outliers here.\n",
    "- __Timestamp is highly cardinal as expected.__\n",
    "- __City__ has 311 unique values and __Calcutta__ tops the list in network usauge.\n",
    "- __State__ has 6 unique values and __West Bengal__ tops the list.\n",
    "- __There are 2 gender types. Male users are more than Female users, approximately 64:36 ratio respectively.__\n",
    "- There are __12 age groups__. Most of the users are Males in age group of 32-38 years.\n",
    "- There are __91 phone brands__ used in these 6 states and __小米 (Xiaomi)__ is the highly used phone brand.\n",
    "- There are __1092 device models__ used in these 6 states and __红米note (Redmi Note)__ is the highly used phone model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section53></a>\n",
    "### **5.3 Pre-Profiling Report**\n",
    "\n",
    "- For **quick analysis** pandas profiling is very handy.\n",
    "\n",
    "- Generates profile reports from a pandas DataFrame.\n",
    "\n",
    "- For each column **statistics** are presented in an interactive HTML report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df = df_final_merged)\n",
    "profile.to_file(output_file = 'CDF Capstone Pre Profiling Report.html')\n",
    "print('Accomplished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations from Pandas Profiling before Data Processing__<br><br>\n",
    "__Dataset info__:\n",
    "- Number of variables: 12\n",
    "- Number of observations: 422971\n",
    "- Missing cells: 84\n",
    "- No duplicate rows:0\n",
    "\n",
    "__Variables types__: \n",
    "- Numeric = 5\n",
    "- Categorical = 7\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "- event id is unique as expected.\n",
    "- device id has 19032 distinct values (4.5%) and no missing or zero values present.\n",
    "- Timestamp is highly cardinal and uniform. It has 68.6% distinct values.\n",
    "- longitude and latitude have 42 missing values each which is < 0.1%\n",
    "- City has 311 distinct values. Calcutta with highest frequency 28.9% followwed by Bangalore 11.8%. Rest other cities are <0.5%\n",
    "- State has 6 distinct values. WestBengal with highest frequency 46.4% followwed by Karnataka 23.4%. Rest other states are <10%\n",
    "- Gender has 2 distinct values. Male is 64.3% and Female 35.7%. It is highly correlated with group column.\n",
    "- Age distribution is right skewed. Mean age is 31.4.\n",
    "- For full details check out the report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section54></a>\n",
    "### **5.4 Handling of Missing Data**\n",
    "\n",
    "- In this section we will **handle** **missing information** such as **null data** and **zero data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Handling the missing values for longitude and latitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['longitude'].isnull()].groupby('device_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['device_id'] == 1320050977019711232].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_final_merged[(df_final_merged['device_id'] == 1320050977019711232) & (df_final_merged['longitude'].isnull())].groupby(['device_id','city','state']).count()\n",
    "df_final_merged[(df_final_merged['device_id'] == 1320050977019711232)].groupby(['device_id','city','state','longitude','latitude']).count()\n",
    "#df_final_merged[df_final_merged['device_id'] == 1320050977019711232].groupby(['device_id','city','state']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is only 1 longitude & latitude for device id 1320050977019711232 for 482 records and rest of 14 records are null. So we can fill this logitude 87.57074 & latitude 26.21192 for this device id's missing longitude and latitude.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.loc[((df_final_merged['city'] == 'Araria') & (df_final_merged['state'] == 'Bihar') & (df_final_merged['device_id'] == 1320050977019711232)) & (df_final_merged['longitude'].isnull()), 'longitude'] = 87.57074\n",
    "df_final_merged.loc[((df_final_merged['city'] == 'Araria') & (df_final_merged['state'] == 'Bihar') & (df_final_merged['device_id'] == 1320050977019711232)) & (df_final_merged['latitude'].isnull()), 'latitude'] = 26.21192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['longitude'].isnull()].groupby('device_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['device_id'] == 3099168546198768640].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[(df_final_merged['device_id'] == 3099168546198768640)].groupby(['device_id','city','state','longitude','latitude']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is only 1 longitude & latitude for device id 3099168546198768640 for 533 records and rest of 14 records are null. So we can fill this logitude 84.14090 & latitude 27.17740 for this device id's missing longitude and latitude.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.loc[((df_final_merged['city'] == 'Bagaha') & (df_final_merged['state'] == 'Bihar') & (df_final_merged['device_id'] == 3099168546198768640)) & (df_final_merged['longitude'].isnull()), 'longitude'] = 84.14090\n",
    "df_final_merged.loc[((df_final_merged['city'] == 'Bagaha') & (df_final_merged['state'] == 'Bihar') & (df_final_merged['device_id'] == 3099168546198768640)) & (df_final_merged['latitude'].isnull()), 'latitude'] = 27.17740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['longitude'].isnull()].groupby('device_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['device_id'] == 6774071338248978432].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[(df_final_merged['device_id'] == 6774071338248978432)].groupby(['device_id','city','state','longitude','latitude']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is only 1 longitude & latitude for device id 6774071338248978432 for 444 records and rest of 14 records are null. So we can fill this logitude 75.26875 & latitude 30.90418 for this device id's missing longitude and latitude.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values\n",
    "df_final_merged.loc[((df_final_merged['city'] == 'Moga') & (df_final_merged['state'] == 'Punjab') & (df_final_merged['device_id'] == 6774071338248978432)) & (df_final_merged['longitude'].isnull()), 'longitude'] = 75.26875\n",
    "df_final_merged.loc[((df_final_merged['city'] == 'Moga') & (df_final_merged['state'] == 'Punjab') & (df_final_merged['device_id'] == 6774071338248978432)) & (df_final_merged['latitude'].isnull()), 'latitude'] = 30.90418\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged[df_final_merged['longitude'].isnull()].groupby('device_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Phone brand name conversion** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the unique phone branpd.\n",
    "df_final_merged['phone_brand_english'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section54></a>\n",
    "### **5.4 Feature Engineering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section55></a>\n",
    "### **5.5 Post Processing Report**\n",
    "\n",
    "- After doing **missing value Imputation**, **feature engineering**, **Removing unwanted features** we will now look at the report again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section6></a>\n",
    "\n",
    "---\n",
    "# **6. Exploratory Data Analysis**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Question: </h4>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section7></a>\n",
    "\n",
    "---\n",
    "# **7. Post Data Processing & Feature Selection**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section71></a>\n",
    "### **7.1 Feature Selection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section72></a>\n",
    "### **7.2 Encoding Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section73></a>\n",
    "### **7.3 Data Preparation**\n",
    "\n",
    "- Now we will **split** our **data** in **training** and **testing** part for further development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section8></a>\n",
    "\n",
    "---\n",
    "# **8. Model Development & Evaluation**\n",
    "---\n",
    "\n",
    "- In this section we will **develop xxModel namexxx using input features** and **tune** our **model if required**.\n",
    "\n",
    "- Then we will **analyze the results** obtained and **make our observation**.\n",
    "\n",
    "- For **evaluation purpose** we will **focus** on **Accuracy**, Also we will check for **Precision**,**Recall**,**F1-Score**,**Roc-Auc-Curve** and **Precision-Recall Curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section81></a>\n",
    "\n",
    "## **8.1 Model Name - Baseline Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section82></a>\n",
    "\n",
    "## **8.2 Using Trained Model for Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section83></a>\n",
    "\n",
    "## **8.3 Model Name  Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section9></a>\n",
    "\n",
    "---\n",
    "# **9. Conclusion**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section91></a>\n",
    "### **9.1 Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Section92></a>\n",
    "### **9.2 Actionable Insights**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "451.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
